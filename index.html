<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name=viewport content="width=800">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    
    a {
      color: #1772d0;
      text-decoration: none;
    }
    
    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }
    
    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
    }
    
    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
    }
    
    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 22px;
    }
    
    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 16px;
      font-weight: 800;
      color: green;
    }
    
    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
    }
    
    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }
    
    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="images/rrclogo.png">
  <title>Junaid Ahmed Ansari</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>

<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="67%" valign="middle">
              <p align="center">
                <name>Junaid Ahmed Ansari</name>
              </p>
              <!--
              <p>I am a staff research scientist at <a href="https://ai.google/research">Google Research</a>, where I work on computer vision and computational photography. At Google I've worked on <a href="https://ai.googleblog.com/2014/04/lens-blur-in-new-google-camera-app.html">Lens Blur</a>, <a href="https://ai.googleblog.com/2014/10/hdr-low-light-and-high-dynamic-range.html">HDR+</a>, <a href="https://www.google.com/get/cardboard/jump/">Jump</a>, <a href="https://ai.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html">Portrait Mode</a>, and <a href="https://www.youtube.com/watch?v=JSnB06um5r4">Glass</a>.
              </p>
          	-->
		    <p>I am a Researcher at the <stron> Embedded Systems and Robotics group</strong> in TCS Innovation Labs, Kolkata, India. Previously, I was a research Masters student at <a href="https://robotics.iiit.ac.in/">Robotics Research Center</a> at <a href="https://www.iiit.ac.in/"> IIIT Hyderabad </a>, India, advised by <a href="https://scholar.google.co.in/citations?user=QDuPGHwAAAAJ&hl=en">Prof. K. Madhava Krishna</a>.
		   I was <a href="https://www.qualcomm.com/company/locations/india/news/qualcomm-announces-winners-2017-qualcomm-innovation-fellowship-program-india">QInF Fellow </a>(Qualcomm India) for the year 2017-2018. 
			I am interested in computer/machine vision and deep learning. In particualr, my interests span <strong>visual SLAM</strong>, <strong>trajectory prediction</strong>, <strong>multi-object tracking</strong>, <strong>3D reconstruction</strong>; especially in a monocular setting. 
          	</p>
		<!--    
          	<p>	
			I am particularly interested in applying <em>classical/geometric</em> techniques 
			(such as bundle adjustment and pose-graph optimization) in conjunction with <em>deep learning</em> 
			(for tasks like semantic keypoint inference and single view depth prediction) to solve interesting/challenging
			machine vision problems such as dense visual SLAM in highly dynamic environment, motion forecasting of humans/objects, etc..
          	-->	
          		<!-- 
          			I am specifically interested in applying classical techniques (such as bundle adjustment and pose-graph optimization) in conjunction with deep learning (for tasks like semantic keypoint inference and single view depth prediction) to rather bring the dynamism of the scene in the (monocular visual) SLAM systems, and not ignore it. 
          		-->
              </p>
              <p>              	
		      My masters thesis focuses on <strong>reconstruction of moving (or static) vehicles on arbitrary road plane
		      profiles from a moving monocular camera</strong>. I leverage feature prediction capabilities of deep networks tightly coupled with 
		      classical geometric methods such as multi-view stereo and bundle adjustment to jointly optimize the pose-shape of the 
		      vehicle and the local road plane geometry.  
              </p>


              <p align=center>                
                <a href="mailto:ansariahmedjunaid@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://github.com/JunaidCS032">GitHub</a> &nbsp/&nbsp
                <a href="data/junaid-cv.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.co.in/citations?user=Uc8mKqMAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/junaidcs032/"> LinkedIn </a>
              </p>
            </td>
            <td width="33%">
              <img src="images/junaid_ahmed.jpg" width="180" height="234">
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Research</heading>             

            </td>
          </tr>
        </table>
	      
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
         <tr>
            <td width="25%">
              <div class="one">                
                <img src='images/iros2020.gif' width="170" height="170">
              </div>              
            </td>
            <td valign="middle" width="75%">
              
                <papertitle>Simple means Faster: Real-Time Human Motion Forecasting in Monocular First Person Videos on CPU</papertitle>              
              <br>              
              <strong>Junaid Ahmed Ansari</strong>,
              <a href="https://sites.google.com/view/brojeshwar/home">Brojeshwar Bhowmick</a>            
              <br>
              <em>IROS</em>, 2020 &nbsp <font color="red"></font>
              <br>
		    <a href="http://ras.papercept.net/images/temp/IROS/files/1944.pdf"> Paper</a> / Video 
		<p></p>
              <p>In this work we present a simple, fast, and light-weight RNN based framework for forecasting future locations of humans in first person monocular videos. The primary motivation for
this work was to design a network which could accurately predict future trajectories at a very high rate of ~78 trajectories per second on a CPU.</p>
            </td>
          </tr>
		
	  <tr>
            <td width="25%">
              <div class="one">                
                <img src='images/iv2020.gif' width="170" height="170">
              </div>              
            </td>
            <td valign="middle" width="75%">
              
                <papertitle>Multi-object Monocular SLAM for Dynamic Environments</papertitle>              
              <br>              
		    <a href="http://gokulbnr.ml/">Gokul B. Nair</a>, Swapnil Daga, Rahul Sajnani, Anirudha Ramesh, <strong>Junaid Ahmed Ansari</strong>, K. Madhava Krishna,                  
              <br>
              <em>IEEE Intelligent Vehicle Symposium (IV)</em>, 2020 &nbsp <font color="red"></font>
              <br>  		
              <a href="https://arxiv.org/abs/2002.03528">arxiv</a> /
              <a href="https://youtu.be/cchPIaKSSvM">video</a>    
		<p></p>
              <p> In this work, we tackle the problem of multibody SLAM from a monocular camera. The term multibody, implies that we track the motion of the camera, as well as that of other dynamic participants in the scene. 
		  We solve this rather intractable problem by leveraging single-view metrology, advances in deep learning, and category-level shape estimation. We propose a multi pose-graph optimization formulation, to resolve the relative and absolute scale factor ambiguities involved.
		    </p>
            </td>
          </tr>	
		
          <tr onmouseout="hide_back_image1()" onmouseover="show_back_image1()">
            <td width="25%">
              <div class="one">
                <div class="two" id='infer_kitti'><img src='images/infer_transfer.png' width="170" height="170"></div>
                <img src='images/infer_kitti.png' width="170" height="170">
              </div>
              <script type="text/javascript">
                function hide_back_image1() {
                  document.getElementById('infer_kitti').style.opacity = "0";
                }
                function show_back_image1() {
                  document.getElementById('infer_kitti').style.opacity = "1";
                }
                hide_back_image1()
              </script>
            </td>
            <td valign="middle" width="75%">
              
                <papertitle>INFER: INtermediate representations for FuturE pRediction</papertitle>
              
              <br>
              <a href="https://talsperre.github.io/">Shashank Srikanth</a>,
              <strong>Junaid Ahmed Ansari</strong>,
              <a href="http://karnikram.info/">Karnik Ram </a>,
              <a href="https://scholar.google.com/citations?user=4uKV9aIAAAAJ&hl=en">Sarthak Sharma</a>,
              <a href="http://krrish94.github.io/">J Krishna Murthy</a>,
              <a href="https://scholar.google.co.in/citations?user=QDuPGHwAAAAJ&hl=en">K Madhava Krishna</a>            
              <br>
              <em>IROS</em>, 2019 &nbsp <font color="red"></font>
              <br>
              <a href="https://arxiv.org/pdf/1903.10641">arxiv</a> /
              <a href="https://talsperre.github.io/INFER/">project page</a> /
              <a href="https://github.com/talsperre/INFER">code</a> /
              <a href="https://youtu.be/sHxXIX-FZoU">video</a>
			  <p></p>
              <p>In this work we proposed an adequate <i>intermediate representation</i> of the scene that not only facilitates better <b>end-to-end trajectory forecasting,</b>
		      but also allows the model to be transferred <b><i> zero-shot </i></b> to other datasets.</p>
            </td>
          </tr>

          <tr onmouseout="show_back_image2()" onmouseover="hide_back_image2()">
            <td width="25%">
              <div class="one">
                <div class="two" id='iros18_kitti'><img src='images/iros18_synthia.png' width="170" height="170"></div>
                <img src='images/iros18_kitti.png' width="170" height="170">
              </div>
              <script type="text/javascript">
                function hide_back_image2() {
                  document.getElementById('iros18_kitti').style.opacity = "0";
                }
                function show_back_image2() {
                  document.getElementById('iros18_kitti').style.opacity = "1";
                }
                hide_back_image2()
              </script>
            </td>
            <td valign="middle" width="75%">
              
                <papertitle>The Earth ain't Flat: Monocular reconstruction of Vehicles on Steep and Graded Roads from
			    a Moving Camera</papertitle>
              
              <br>              
              <strong>Junaid Ahmed Ansari*</strong>,              
              <a href="https://scholar.google.com/citations?user=4uKV9aIAAAAJ&hl=en">Sarthak Sharma*</a>,
              <a href="http://krrish94.github.io/">J Krishna Murthy</a>,
              <a href="https://scholar.google.co.in/citations?user=QDuPGHwAAAAJ&hl=en">K Madhava Krishna</a>            
              <br>
              <em>IROS</em>, 2018 &nbsp <font color="red"></font> (*equal contribution)
              <br>
              <a href="data/IROS18_1539_FI.pdf">arxiv</a> /               
              <a href="https://www.youtube.com/watch?v=C_FKg0HTfw4">video </a>/ 
              code (coming soon)
			  <p></p>
              <p>In this work we estimate the pose-shape of moving (or static) vehicles on arbitrary road plane profiles. We 
		      proposed a joint optimization framework that couples the pose-shape of the target vehicle with its
		       local road plane geometry and thereby relaxing a widely exploited assumption of coplanarity that
		       states that the vehicles of interest and the ego-vehicle share the same road plane.</p>
            </td>
          </tr>
      

      	  <tr onmouseout="hide_back_image3()" onmouseover="show_back_image3()">
            <td width="25%">
              <div class="one">
                <div class="two" id='icra18'><img src='images/icra18_2.png' width="170" height="170"></div>
                <img src='images/icra18_1.png' width="170" height="170">
              </div>
              <script type="text/javascript">
                function show_back_image3() {
                  document.getElementById('icra18').style.opacity = "1";
                }
                function hide_back_image3() {
                  document.getElementById('icra18').style.opacity = "0";
                }
                hide_back_image3()
              </script>
            </td>
            <td valign="middle" width="75%">
              
                <papertitle>Beyond Pixels: Leveraging Geometry and Shape Cues for Online Multi-Object tracking</papertitle>
              
              <br>              
              <a href="https://scholar.google.com/citations?user=4uKV9aIAAAAJ&hl=en">Sarthak Sharma*</a>,
              <strong>Junaid Ahmed Ansari*</strong>,                           
              <a href="http://krrish94.github.io/">J Krishna Murthy</a>,
              <a href="https://scholar.google.co.in/citations?user=QDuPGHwAAAAJ&hl=en">K Madhava Krishna</a>            
              <br>
              <em>ICRA</em>, 2018 &nbsp <font color="red"></font> (*equal contribution)
              <br>
              <a href="https://arxiv.org/pdf/1802.09298.pdf">arxiv</a> / 
              <a href="https://junaidcs032.github.io/Geometry_ObjectShape_MOT/">project page</a> /
              <a href="https://github.com/JunaidCS032/MOTBeyondPixels">code</a> /
              <a href="https://www.youtube.com/watch?v=2yApZOv_VkU">video / </a>   
              <a href="http://www.cvlibs.net/datasets/kitti/eval_tracking_detail.php?result=43713ff142271660c77d258af6bdbceb079db678">KITTI Tracking Evaluation </a>
			  <p></p>
              <p>Simple and complementary cues were proposed for online tracking of cars using monocular image sequences. Our method was state-of-the-art<sup>+</sup> on KITTI Tracking benchmark at the time of submission.
              <br>
              <i> <sup>+</sup>amongst the published approaches.</i></p>
            </td>
          </tr>

          <tr>
            <td width="25%">
              <div class="one">                
                <img src='images/vcik_framework.png' width="170" height="170">
              </div>              
            </td>
            <td valign="middle" width="75%">
              
                <papertitle>An Open Voice Command Interface Kit</papertitle>
              
              <br>                           
              <strong>Junaid Ahmed Ansari</strong>,                           
              <a href="http://www.rri.res.in/ral_arasi.html">Arasi Sathyamurthi</a>,
              <a href="http://www.rri.res.in/aa_rameshb.html">Ramesh Balasubramanyam</a>            
              <br>
              <em>IEEE Transactions on Human-Machine Systems</em>, 2016 &nbsp <font color="red"></font>
              <br>
              <a href="https://ieeexplore.ieee.org/document/7300400">paper</a> / 
              <a href="https://github.com/projectopenvcik/OpenVCIK/wiki/OpenVCIK-Services">project page</a> /
              <a href="https://github.com/projectopenvcik/OpenVCIK">code</a> /
              <a href="https://youtu.be/7SkeCymsgMI">video </a>
			  <p></p>
              <p>This voice command interface kit, developed by integrating open-source software and inexpensive hardware components, can be used for a variety of applications such as home automation, device control, and robot control; in this case we are controlling an electric wheelchair via voice commands. The software for the kit has been developed as a lightweight, multi-threaded and modular framework with replaceable components. It also provides a C++ library to support development of applications on top of it. </p>       
            </td>
          </tr>	

          <tr>
            <td width="25%">
              <div class="one">                
                <img src='images/vacu_boards.png' width="170" height="170">
              </div>              
            </td>
            <td valign="middle" width="75%">
              
                <papertitle>VACU - Voice Activated Control Unit</papertitle>
              
              <br>                           
              <a href="http://www.rri.res.in/ral_arasi.html">Arasi Sathyamurthi</a>,
              <strong>Junaid Ahmed Ansari</strong>,                                         
              <a href="http://www.rri.res.in/aa_rameshb.html">Ramesh Balasubramanyam</a>            
              <br>
              <em>Poster in Indo-German workshop on Neurobionics in clinical Neurology</em>, 2012 &nbsp <font color="red"></font>
              <br>
              <a href="https://drive.google.com/open?id=0B-9NOTtQ3zTQbkREcmZLWW1rc00">poster</a> / 
              <a href="https://youtu.be/7SkeCymsgMI">video </a>
			  <p></p>
              <p>
              	VACU is an innovative, inexpensive, and standalone embedded solution for voice activation of a powered wheelchair to enable physically challenged people become self-reliant for their locomotion. It is completely based on COTS hardware and provides interfaces for thumb-sticks, sonar sensors,and a compass sensor with PID control for steady motion and collision avoidance; it also has a visual and auditory feedback interface
              <br> </p>
            </td>
          </tr>

        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>Selected Projects</heading>  <br>
              Please have a look at my CV for a complete list of projects.            
            </td>
          </tr>
        </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

          <tr>
            <td width="25%">
              <div class="one">                
                <img src='images/cair_dir.png' width="170" height="170">
              </div>
            </td>
            <td valign="middle" width="75%">
              
                <papertitle>Safe and feasible frontier detection for autonomous ground vehicles</papertitle>
              
              <br>                                         
              <strong>Junaid Ahmed Ansari</strong>,  
              <a href="https://scholar.google.co.in/citations?user=QDuPGHwAAAAJ&hl=en">K Madhava Krishna</a>

              <br>
              <em>Multi-robot SLAM project (DRDO-CAIR)</em>, 2016 &nbsp <font color="red"></font>
              <br>              
              <a href="data/frontier_detection_project_report.pdf">report</a> /               
              <a href="https://youtu.be/X2fnDjdAOKk">video</a> / 
              code (coming soon)
			  <p></p>
              <p>
              	Developed a ROS (C++) package for detection of safe and feasible frontiers for autonomous ground vehicles. Obstacles are segmentated by fitting road plane (along with camera height information) to the 3D point cloud generated using a stereo camera. Based on the obstacle information, vehicle dimension, frontier direction, and traversibility we compute all possible headings which are safe and feasible for the robot to move in.
              <br> </p>
            </td>
          </tr>	          	

          <tr onmouseout="hide_back_image8()" onmouseover="show_back_image8()">
            <td width="25%">
              <div class="one">
                <div class="two" id='drawinair'><img src='images/drawinair_output.png' width="170" height="170"></div>
                <img src='images/drawinair_img.png' width="170" height="170">
              </div>
              <script type="text/javascript">
                function show_back_image8() {
                  document.getElementById('drawinair').style.opacity = "1";
                }
                function hide_back_image8() {
                  document.getElementById('drawinair').style.opacity = "0";
                }
                hide_back_image8()
              </script>
            </td>
            <td valign="middle" width="75%">
              
                <papertitle>Draw in Air</papertitle>
              <br>
              <em>Image processing project</em>
              <br>                            
              <a href="https://drive.google.com/drive/u/0/folders/0B-9NOTtQ3zTQTUxZcmJ4Z25pMTQ"> code / </a>
              <a href="https://www.youtube.com/watch?v=8y4feqa-6pw">video </a>
			  <p></p>
              <p>
              	Draw-In-Air is an application for drawing, capturing images and controlling the mouse by color marker based gestures. Developed an algorithm for recognizing simple gestures employing two color markers for image capture
              <br> </p>
            </td>
          </tr>	
	  <tr onmouseout="hide_back_image7()" onmouseover="show_back_image7()">
            <td width="25%">
              <div class="one">
                <div class="two" id='corridor'><img src='images/corridor.png' width="170" height="170"></div>
                <img src='images/corridor_detected.png' width="170" height="170">
              </div>
              <script type="text/javascript">
                function show_back_image7() {
                  document.getElementById('corridor').style.opacity = "1";
                }
                function hide_back_image7() {
                  document.getElementById('corridor').style.opacity = "0";
                }
                hide_back_image7()
              </script>
            </td>
            <td valign="middle" width="75%">
              
                <papertitle>Corridor detection in point cloud</papertitle>
              
              <br>                            
              <a href="https://drive.google.com/drive/u/0/folders/0B-9NOTtQ3zTQTUxZcmJ4Z25pMTQ"> code </a>
			  <p></p>
              <p>
              	Corridor is detected in the point cloud by looking for a dominant parallel line separated by a distance threshold in a 2D scan generated from the 3D data captured from Kinect sensor. 
		Line segments were extracted using recursive split and merge technique. The project was implemented in MATLAB.
              <br> </p>
            </td>
          </tr>
        </table>        	

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>Fellowships, Awards, and Academic Services</heading>
            </td>
          </tr>
	
        </table>

	      2019 - <strong>Reviewer</strong>, IEEE Intelligent Vehicles (IV) Symposium. <br>
	      2018 - <strong>Teaching Assistant</strong>, CSE483 Mobile Robotics (robotic vision). <br>
	      2017-18 - <strong>Fellow</strong>, Qualcomm Innovation Fellowship (QInF), India. <br>
	      2017 - <strong>Winner</strong>, Qualcomm Innovation Fellowship (QInf), India. <br>
	      2011-2012 - <strong>Fellow</strong>, Visiting Student Programme, Raman Research Institute, Bangalore, India. <br>
	      2010 - <strong>First Prize</strong>, Inter-state C programming competition. Competition organized by IEEE Student Branch,
SVCE, Bangalore, India.



	
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              Thanks to Dr. Jon Baron for the web page <a href="https://github.com/jonbarron/website">template</a>.
            </td>
          </tr>
        </table>
        </td>

    </tr>    
  </table>
</body>

</html>
